# RCNN 论文笔记

[toc]

## 摘要

​	RCNN可以说是利用深度学习进行目标检测的开山之作。在当时的VOC 2012数据集上，将mAP提升了30%以上，达到了53.3%。

​	这篇论文主要提出两个关键见解：

- 可以将高容量的卷积神经网络应用在自底向上的区域建议(region proposal)，以此来定位和分割对象。
- 当训练数据不足时，可以选择进行辅助任务的监督预训练，再针对具体任务进行fine-tuning，以此获得显著的性能提升。

## 算法流程

- 一张图像生成1k~2k个候选区域(使用**Selective Search**)
- 对每个候选区域，使用**深度网络提取特征**
- 特征送入每一类的**SVM分类器**，判别种类
- 使用**回归器**对候选框进行微调

![image](https://user-images.githubusercontent.com/92386084/226543702-80b1a2a0-7b03-417e-92b9-56c9597c8468.png)

### 候选区域生成

​	使用Selective Search算法。算法大致思路是通过聚类的方法在图像上初步分割，找到

颜色/纹理/大小/相似度比较相似的区域，再根据这些区域进行加权合并，最后(希望)得到一些包括了GT box的候选框。
![image](https://user-images.githubusercontent.com/92386084/226543716-c5931c9e-8fc2-41f7-a4f5-c42e5479a5ab.png)

### AlexNet 提取特征

​	对于每个候选框，将框中的内容提取出来，并**缩放至227x227的大小**，送入**AlexNet**中提取特征，产生**4096维的向量**。

> 这里之所以一定要缩放到固定大小，是因为CNN里面有全连接层
![image](https://user-images.githubusercontent.com/92386084/226543739-73cd1a1a-58f0-4838-89bc-5517df86b14f.png)

### SVM分类

​	将2000x4096的特征矩阵与**20个SVM(20个类别)组成的4096x20权值矩阵**相乘，得到2000x20的分数矩阵，行表示每个候选区域，列表是该候选区域对应类别的分数(概率).

​	针对分数矩阵，在每一列中应用**greed NMS**剔除重叠的建议框，得到该列中得分较高的一些候选框。

> 之所以用SVM而不是直接softmax在论文附录中有提到，第一个原因是**训练神经网络和训练SVM时采用的参数(IoU阈值)不一致**，无法直接实现端到端。第二个原因是**softmax无法实现难例挖掘**(存疑)

### 回归器修正候选框位置

​	回归器同样是利用CNN输出的特征向量(4096维)进行预测,通过预测相对于候选框的宽高偏移和位置偏移，可以对候选框的位置进行微调，更接近GT Box。

​	预测时通过**四个函数dx/dy/dw/dh来拟合GT Box**，前两个公式拟合GTBox的中心位置，后两个位置拟合宽高的指数偏移。

![image](https://user-images.githubusercontent.com/92386084/226543763-a62f1ee1-0040-4e25-acac-145f27e4eaee.png)

​	训练时的**标签值**可以由以下公式获得。也就是上面四个公式的逆变换。
![image](https://user-images.githubusercontent.com/92386084/226543789-70f3c25a-6d64-4f63-a427-e701359bde22.png)
### 网络框架
![image](https://user-images.githubusercontent.com/92386084/226543813-3ebe8553-de28-4c31-a87e-b629fcc17d7e.png)

## 卷积可视化

​	论文里提到一个卷积可视化的概念。具体来说，**选择在神经网络的某一个神经元，将他作为一个独立的对象检测器使用**。对于该神经元，输入10 million的region proposal，**找到这些区域中使该神经元值最大的那些候选框**。通过输出这些图像，可以发现每个神经元有自己偏好的识别模式.比如上半身/狗子/红花等等。

![image](https://user-images.githubusercontent.com/92386084/226543828-fe876e22-302d-4487-b290-a2d8dee1ca28.png)

​	论文中也提到这些神经元选取的是POOL5中的神经元。POOL5在网络尾部，所以偏好的识别模式也是比较抽象的(上文提到的上半身等等)。同时论文中也说到**在一个通道内，坐标y/x只影响感受野，不影响识别模式**。(很好理解，卷积核是不变的)

![image](https://user-images.githubusercontent.com/92386084/226543851-84c94b62-320e-4d30-81bd-acabe4e57be4.png)

## 消融实验

​	论文里还做了一些消融实验，**比较了神经网络中卷积层和全连接层的作用**。在不进行fine-tuning时，不加全连接的mAP比较高；加了fine-tuning后，加了全连接的mAP比较高。得出结论：

①在HOG意义上，通过**仅使用CNN的卷积层来计算任意大小的图像的密集特征图方面具有潜在的实用性**。这种表示将使得能够在pool5特征之上使用滑动窗口检测器（包括DPM）进行实验。(其实就是卷积层负责提取通用特征？)

②从ImageNet学习的pool5特征是通用的，并且大部分mAP改善是通过在其上**学习特定的非线性分类器**获得的。(全连接层负责整合特征)

## RCNN存在的问题

- **测试速度慢**，测试一张图片需要53s(CPU),首先需要2秒提取候选框，随后对2k个候选框进行卷积操作(最耗时间)，最后进行矩阵乘法和NMS(以及利用回归器)筛选(回归)候选框。其中神经网络抽取特征耗时最长，因为进行很多重复操作(2k个候选框难免会有很多重叠区域）
- **训练速度慢**，不是端到端的系统，SVM分类器，BBOX回归器都需要单独训练。
- **训练所需空间大**，训练SVM和回归器的时候需要将每个候选框的特征写入磁盘。对于VGG16，从VOC2007训练集上5k个图像提取的特征，需要数百GB的存储空间。
